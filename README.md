# 这个仓库可作为数图文档共享编辑

## <center>2 Approach

### 2.1 概览

> Qwen2.5-VL模型架构整体上分为三个模块，LLM、Vision Encoder与MLP-based Vision-Language Merger，在本节中将概述三个模块的创新做法，在2.1.x中会详细展开

* **旋转位置嵌入**（1D RoPE）改为**多模态旋转位置嵌入**（MRoPE）
  * 旋转位置嵌入即在复数域中进行旋转来编码位置信息，为模型提供每个token的位置信息
  * 多模态旋转位置词嵌入将位置信息分解为***时间Temporal、高度Height、宽度Width***，对于不同类型输入三个分量使用方式不同。例如：对于视频即为宽、高与帧序列，图像与文本输入同理
* 采用重新设计的**ViT**架构
  * 结合2D RoPE与窗口注意力以支持图像原生分辨率输入
  * 图像高宽resize为28的倍数，并切割为`14*14`的patch
* 压缩特征序列再输入LLM
  * 不直接用ViT提取的patch，而是将相邻的四个patch分组的特征连接，通过MLP投影到LLM中文本嵌入对齐的维度中

#### 2.1.1 Fast and Efficient Vision Encoder

* 仅有四层采用了完全自注意力机制，其余都是窗口注意力机制，以降低复杂度；窗口注意力中最大窗口为`112*112`，更小的区域不进行填充，以避免分辨率差异导致的计算负载不均衡
* 位置编码上，`14*14`大小的patch作为输入块，并在视频处理上将**两个连续帧**组合到一起，减少输入词元数
* 为简化网络结构，采用`RMSNorm`归一化，`SwiGLU`作为激活函数，提高计算效率
* 训练过程中，图像根据宽高比**随即采样**，泛化不同分辨率输入

#### 2.1.2 MLP-based Vision-Language Merger
